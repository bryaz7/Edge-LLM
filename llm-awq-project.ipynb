{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12252,"status":"ok","timestamp":1733671846592,"user":{"displayName":"Bryan Duong","userId":"08362964251007024437"},"user_tz":360},"id":"pxGcU5yjYdpR","outputId":"2b1938a7-a1b1-401a-f5d6-3fe9de75e324"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'llm-awq'...\n","remote: Enumerating objects: 881, done.\u001b[K\n","remote: Counting objects: 100% (450/450), done.\u001b[K\n","remote: Compressing objects: 100% (211/211), done.\u001b[K\n","remote: Total 881 (delta 328), reused 243 (delta 239), pack-reused 431 (from 1)\u001b[K\n","Receiving objects: 100% (881/881), 73.24 MiB | 6.69 MiB/s, done.\n","Resolving deltas: 100% (468/468), done.\n"]}],"source":["!git clone https://github.com/mit-han-lab/llm-awq.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":112,"status":"ok","timestamp":1733671882294,"user":{"displayName":"Bryan Duong","userId":"08362964251007024437"},"user_tz":360},"id":"GOS2-tuwZWjz","outputId":"20a1f536-e6cf-4df1-beed-0472fe2d3b31"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/llm-awq\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/llm-awq'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["%cd llm-awq/\n","%pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38561,"status":"ok","timestamp":1733671937733,"user":{"displayName":"Bryan Duong","userId":"08362964251007024437"},"user_tz":360},"id":"IrxDslUIZclx","outputId":"84f85581-7c18-4a7a-aa41-b1efc4676780"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: conda: command not found\n","/bin/bash: line 1: conda: command not found\n","/bin/bash: line 1: conda: command not found\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n","Collecting pip\n","  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n","Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.1.2\n","    Uninstalling pip-24.1.2:\n","      Successfully uninstalled pip-24.1.2\n","Successfully installed pip-24.3.1\n","Obtaining file:///content/llm-awq\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from awq==0.1.0) (1.1.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from awq==0.1.0) (0.2.0)\n","Requirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from awq==0.1.0) (0.20.3)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from awq==0.1.0) (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from awq==0.1.0) (0.20.1+cu121)\n","Collecting transformers==4.37.2 (from awq==0.1.0)\n","  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n","Collecting lm_eval==0.3.0 (from awq==0.1.0)\n","  Downloading lm_eval-0.3.0-py3-none-any.whl.metadata (6.8 kB)\n","Collecting texttable (from awq==0.1.0)\n","  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from awq==0.1.0) (0.10.2)\n","Collecting attributedict (from awq==0.1.0)\n","  Downloading attributedict-0.3.0-py3-none-any.whl.metadata (3.1 kB)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from awq==0.1.0) (4.25.5)\n","Collecting gradio==3.35.2 (from awq==0.1.0)\n","  Downloading gradio-3.35.2-py3-none-any.whl.metadata (15 kB)\n","Collecting gradio_client==0.2.9 (from awq==0.1.0)\n","  Downloading gradio_client-0.2.9-py3-none-any.whl.metadata (7.0 kB)\n","Collecting fastapi (from awq==0.1.0)\n","  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n","Collecting uvicorn (from awq==0.1.0)\n","  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n","Collecting pydantic==2.9.2 (from awq==0.1.0)\n","  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n","Collecting aiofiles (from gradio==3.35.2->awq==0.1.0)\n","  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (3.11.9)\n","Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (4.2.2)\n","Collecting ffmpy (from gradio==3.35.2->awq==0.1.0)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (0.28.0)\n","Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (0.26.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (3.1.4)\n","Requirement already satisfied: markdown-it-py>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.35.2->awq==0.1.0) (3.0.0)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (3.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (3.8.0)\n","Collecting mdit-py-plugins<=0.3.3 (from gradio==3.35.2->awq==0.1.0)\n","  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (1.26.4)\n","Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (3.10.12)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (2.2.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (11.0.0)\n","Collecting pydub (from gradio==3.35.2->awq==0.1.0)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (2.18.0)\n","Collecting python-multipart (from gradio==3.35.2->awq==0.1.0)\n","  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio==3.35.2->awq==0.1.0) (2.32.3)\n","Collecting semantic-version (from gradio==3.35.2->awq==0.1.0)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting websockets>=10.0 (from gradio==3.35.2->awq==0.1.0)\n","  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio_client==0.2.9->awq==0.1.0) (2024.10.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio_client==0.2.9->awq==0.1.0) (24.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio_client==0.2.9->awq==0.1.0) (4.12.2)\n","Collecting datasets>=2.0.0 (from lm_eval==0.3.0->awq==0.1.0)\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Collecting jsonlines (from lm_eval==0.3.0->awq==0.1.0)\n","  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.3.0->awq==0.1.0) (2.10.2)\n","Requirement already satisfied: openai>=0.6.4 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.3.0->awq==0.1.0) (1.54.5)\n","Collecting pybind11>=2.6.2 (from lm_eval==0.3.0->awq==0.1.0)\n","  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n","Collecting pycountry (from lm_eval==0.3.0->awq==0.1.0)\n","  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n","Collecting pytablewriter (from lm_eval==0.3.0->awq==0.1.0)\n","  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\n","Collecting rouge-score>=0.0.4 (from lm_eval==0.3.0->awq==0.1.0)\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting sacrebleu==1.5.0 (from lm_eval==0.3.0->awq==0.1.0)\n","  Downloading sacrebleu-1.5.0-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.3.0->awq==0.1.0) (1.5.2)\n","Collecting sqlitedict (from lm_eval==0.3.0->awq==0.1.0)\n","  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tqdm-multiprocess (from lm_eval==0.3.0->awq==0.1.0)\n","  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n","Collecting zstandard (from lm_eval==0.3.0->awq==0.1.0)\n","  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.9.2->awq==0.1.0) (0.7.0)\n","Collecting pydantic-core==2.23.4 (from pydantic==2.9.2->awq==0.1.0)\n","  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2->awq==0.1.0) (3.16.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2->awq==0.1.0) (2024.9.11)\n","Collecting tokenizers>=0.12.1 (from awq==0.1.0)\n","  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2->awq==0.1.0) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2->awq==0.1.0) (4.66.6)\n","Collecting portalocker (from sacrebleu==1.5.0->lm_eval==0.3.0->awq==0.1.0)\n","  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->awq==0.1.0) (3.4.2)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->awq==0.1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->awq==0.1.0) (1.3.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->awq==0.1.0) (8.1.7)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->awq==0.1.0) (0.14.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->awq==0.1.0) (5.9.5)\n","Collecting rootpath>=0.1.0 (from attributedict->awq==0.1.0)\n","  Downloading rootpath-0.1.1-py3-none-any.whl.metadata (6.3 kB)\n","Collecting inspecta>=0.1.0 (from attributedict->awq==0.1.0)\n","  Downloading inspecta-0.1.3-py3-none-any.whl.metadata (3.7 kB)\n","Collecting colour-runner>=0.0.5 (from attributedict->awq==0.1.0)\n","  Downloading colour_runner-0.1.1-py2.py3-none-any.whl.metadata (828 bytes)\n","Collecting deepdiff>=3.3.0 (from attributedict->awq==0.1.0)\n","  Downloading deepdiff-8.0.1-py3-none-any.whl.metadata (8.5 kB)\n","Collecting tox>=3.0.0 (from attributedict->awq==0.1.0)\n","  Downloading tox-4.23.2-py3-none-any.whl.metadata (3.7 kB)\n","Collecting coverage>=4.5.2 (from attributedict->awq==0.1.0)\n","  Downloading coverage-7.6.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n","Collecting codecov>=2.0.15 (from attributedict->awq==0.1.0)\n","  Downloading codecov-2.1.13-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting starlette<0.42.0,>=0.40.0 (from fastapi->awq==0.1.0)\n","  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.35.2->awq==0.1.0) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.35.2->awq==0.1.0) (4.23.0)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.35.2->awq==0.1.0) (0.12.1)\n","Collecting blessings (from colour-runner>=0.0.5->attributedict->awq==0.1.0)\n","  Downloading blessings-1.7-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->lm_eval==0.3.0->awq==0.1.0) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.0.0->lm_eval==0.3.0->awq==0.1.0)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting xxhash (from datasets>=2.0.0->lm_eval==0.3.0->awq==0.1.0)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets>=2.0.0->lm_eval==0.3.0->awq==0.1.0)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec (from gradio_client==0.2.9->awq==0.1.0)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Collecting orderly-set==5.2.2 (from deepdiff>=3.3.0->attributedict->awq==0.1.0)\n","  Downloading orderly_set-5.2.2-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from inspecta>=0.1.0->attributedict->awq==0.1.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from inspecta>=0.1.0->attributedict->awq==0.1.0) (2.5.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.0.0->markdown-it-py[linkify]>=2.0.0->gradio==3.35.2->awq==0.1.0) (0.1.2)\n","Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.35.2->awq==0.1.0) (2.0.3)\n","INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n","Collecting mdit-py-plugins<=0.3.3 (from gradio==3.35.2->awq==0.1.0)\n","  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n","  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n","  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n","  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl.metadata (2.8 kB)\n","  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl.metadata (2.8 kB)\n","  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n","  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl.metadata (2.6 kB)\n","INFO: pip is still looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n","  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl.metadata (2.6 kB)\n","  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n","  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n","  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n","  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl.metadata (2.6 kB)\n","Collecting markdown-it-py[linkify]>=2.0.0 (from gradio==3.35.2->awq==0.1.0)\n","  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n","  Downloading markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=0.6.4->lm_eval==0.3.0->awq==0.1.0) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=0.6.4->lm_eval==0.3.0->awq==0.1.0) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=0.6.4->lm_eval==0.3.0->awq==0.1.0) (0.8.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=0.6.4->lm_eval==0.3.0->awq==0.1.0) (1.3.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.35.2->awq==0.1.0) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.35.2->awq==0.1.0) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.35.2->awq==0.1.0) (3.10)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.35.2->awq==0.1.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.35.2->awq==0.1.0) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.35.2->awq==0.1.0) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gradio==3.35.2->awq==0.1.0) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio==3.35.2->awq==0.1.0) (2.2.3)\n","Collecting coloredlogs>=10.0 (from rootpath>=0.1.0->attributedict->awq==0.1.0)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.3.0->awq==0.1.0) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.3.0->awq==0.1.0) (3.9.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.3.0->awq==0.1.0) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.3.0->awq==0.1.0) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.3.0->awq==0.1.0) (3.5.0)\n","Requirement already satisfied: cachetools>=5.5 in /usr/local/lib/python3.10/dist-packages (from tox>=3.0.0->attributedict->awq==0.1.0) (5.5.0)\n","Requirement already satisfied: chardet>=5.2 in /usr/local/lib/python3.10/dist-packages (from tox>=3.0.0->attributedict->awq==0.1.0) (5.2.0)\n","Collecting colorama>=0.4.6 (from tox>=3.0.0->attributedict->awq==0.1.0)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: platformdirs>=4.3.6 in /usr/local/lib/python3.10/dist-packages (from tox>=3.0.0->attributedict->awq==0.1.0) (4.3.6)\n","Requirement already satisfied: pluggy>=1.5 in /usr/local/lib/python3.10/dist-packages (from tox>=3.0.0->attributedict->awq==0.1.0) (1.5.0)\n","Collecting pyproject-api>=1.8 (from tox>=3.0.0->attributedict->awq==0.1.0)\n","  Downloading pyproject_api-1.8.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from tox>=3.0.0->attributedict->awq==0.1.0) (2.2.1)\n","Collecting virtualenv>=20.26.6 (from tox>=3.0.0->attributedict->awq==0.1.0)\n","  Downloading virtualenv-20.28.0-py3-none-any.whl.metadata (4.4 kB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->awq==0.1.0) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->awq==0.1.0) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->awq==0.1.0) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->awq==0.1.0) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->awq==0.1.0) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->awq==0.1.0) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->awq==0.1.0) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.35.2->awq==0.1.0) (1.18.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.35.2->awq==0.1.0) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.35.2->awq==0.1.0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.35.2->awq==0.1.0) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.35.2->awq==0.1.0) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.35.2->awq==0.1.0) (3.2.0)\n","Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm_eval==0.3.0->awq==0.1.0) (75.1.0)\n","Collecting DataProperty<2,>=1.0.1 (from pytablewriter->lm_eval==0.3.0->awq==0.1.0)\n","  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\n","Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.3.0->awq==0.1.0)\n","  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\n","Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.3.0->awq==0.1.0)\n","  Downloading pathvalidate-3.2.1-py3-none-any.whl.metadata (12 kB)\n","Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.3.0->awq==0.1.0)\n","  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\n","Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.3.0->awq==0.1.0)\n","  Downloading tcolorpy-0.1.6-py3-none-any.whl.metadata (6.4 kB)\n","Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.3.0->awq==0.1.0)\n","  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=0.6.4->lm_eval==0.3.0->awq==0.1.0) (1.2.2)\n","Collecting humanfriendly>=9.1 (from coloredlogs>=10.0->rootpath>=0.1.0->attributedict->awq==0.1.0)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.35.2->awq==0.1.0) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.35.2->awq==0.1.0) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.35.2->awq==0.1.0) (0.22.3)\n","Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.35.2->awq==0.1.0) (1.0.3)\n","Collecting distlib<1,>=0.3.7 (from virtualenv>=20.26.6->tox>=3.0.0->attributedict->awq==0.1.0)\n","  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n","Downloading gradio-3.35.2-py3-none-any.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-0.2.9-py3-none-any.whl (288 kB)\n","Downloading lm_eval-0.3.0-py3-none-any.whl (178 kB)\n","Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n","Downloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n","Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n","Downloading attributedict-0.3.0-py3-none-any.whl (14 kB)\n","Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n","Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n","Downloading codecov-2.1.13-py2.py3-none-any.whl (16 kB)\n","Downloading colour_runner-0.1.1-py2.py3-none-any.whl (3.7 kB)\n","Downloading coverage-7.6.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (234 kB)\n","Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","Downloading deepdiff-8.0.1-py3-none-any.whl (82 kB)\n","Downloading orderly_set-5.2.2-py3-none-any.whl (11 kB)\n","Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","Downloading inspecta-0.1.3-py3-none-any.whl (9.2 kB)\n","Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n","Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n","Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n","Downloading rootpath-0.1.1-py3-none-any.whl (15 kB)\n","Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n","Downloading tox-4.23.2-py3-none-any.whl (166 kB)\n","Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n","Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n","Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n","Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n","Downloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n","Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n","Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m127.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","Downloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n","Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n","Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","Downloading pathvalidate-3.2.1-py3-none-any.whl (23 kB)\n","Downloading pyproject_api-1.8.0-py3-none-any.whl (13 kB)\n","Downloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n","Downloading tcolorpy-0.1.6-py3-none-any.whl (8.1 kB)\n","Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n","Downloading virtualenv-20.28.0-py3-none-any.whl (4.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading blessings-1.7-py3-none-any.whl (18 kB)\n","Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n","Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","Building wheels for collected packages: awq, rouge-score, sqlitedict\n","  Building editable for awq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for awq: filename=awq-0.1.0-0.editable-py3-none-any.whl size=9617 sha256=de63fd1d7e1b53d385a22bd411fc496b7880621ff5e916695fe9bdaa8af16739\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-tccqpkyi/wheels/26/98/ac/1026637af772b6744fe73f3517805cd22f60eb8963bb93ef31\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=d6222df9660d9ff2acf676f815da9afd73d43494bd9da02ff9764c05fa2636b4\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=f281406821e2f8fe90f4e0946578ac3be7a8153aa4253cd42f20fc4b2aeeaaf9\n","  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n","Successfully built awq rouge-score sqlitedict\n","Installing collected packages: texttable, sqlitedict, pydub, distlib, zstandard, xxhash, websockets, virtualenv, uvicorn, tcolorpy, semantic-version, python-multipart, pyproject-api, pydantic-core, pycountry, pybind11, portalocker, pathvalidate, orderly-set, mbstrdecoder, markdown-it-py, jsonlines, humanfriendly, fsspec, ffmpy, dill, coverage, colorama, blessings, aiofiles, typepy, tqdm-multiprocess, tox, starlette, sacrebleu, rouge-score, pydantic, multiprocess, mdit-py-plugins, deepdiff, colour-runner, coloredlogs, codecov, tokenizers, rootpath, gradio_client, fastapi, transformers, inspecta, DataProperty, tabledata, gradio, datasets, attributedict, pytablewriter, lm_eval, awq\n","  Attempting uninstall: pydantic-core\n","    Found existing installation: pydantic_core 2.27.1\n","    Uninstalling pydantic_core-2.27.1:\n","      Successfully uninstalled pydantic_core-2.27.1\n","  Attempting uninstall: markdown-it-py\n","    Found existing installation: markdown-it-py 3.0.0\n","    Uninstalling markdown-it-py-3.0.0:\n","      Successfully uninstalled markdown-it-py-3.0.0\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 2.10.3\n","    Uninstalling pydantic-2.10.3:\n","      Successfully uninstalled pydantic-2.10.3\n","  Attempting uninstall: mdit-py-plugins\n","    Found existing installation: mdit-py-plugins 0.4.2\n","    Uninstalling mdit-py-plugins-0.4.2:\n","      Successfully uninstalled mdit-py-plugins-0.4.2\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.20.3\n","    Uninstalling tokenizers-0.20.3:\n","      Successfully uninstalled tokenizers-0.20.3\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.46.3\n","    Uninstalling transformers-4.46.3:\n","      Successfully uninstalled transformers-4.46.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n","sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed DataProperty-1.0.1 aiofiles-24.1.0 attributedict-0.3.0 awq-0.1.0 blessings-1.7 codecov-2.1.13 colorama-0.4.6 coloredlogs-15.0.1 colour-runner-0.1.1 coverage-7.6.9 datasets-3.1.0 deepdiff-8.0.1 dill-0.3.8 distlib-0.3.9 fastapi-0.115.6 ffmpy-0.4.0 fsspec-2024.9.0 gradio-3.35.2 gradio_client-0.2.9 humanfriendly-10.0 inspecta-0.1.3 jsonlines-4.0.0 lm_eval-0.3.0 markdown-it-py-2.2.0 mbstrdecoder-1.1.3 mdit-py-plugins-0.3.3 multiprocess-0.70.16 orderly-set-5.2.2 pathvalidate-3.2.1 portalocker-3.0.0 pybind11-2.13.6 pycountry-24.6.1 pydantic-2.9.2 pydantic-core-2.23.4 pydub-0.25.1 pyproject-api-1.8.0 pytablewriter-1.2.0 python-multipart-0.0.19 rootpath-0.1.1 rouge-score-0.1.2 sacrebleu-1.5.0 semantic-version-2.10.0 sqlitedict-2.1.0 starlette-0.41.3 tabledata-1.3.3 tcolorpy-0.1.6 texttable-1.7.0 tokenizers-0.15.2 tox-4.23.2 tqdm-multiprocess-0.0.11 transformers-4.37.2 typepy-1.3.2 uvicorn-0.32.1 virtualenv-20.28.0 websockets-14.1 xxhash-3.5.0 zstandard-0.23.0\n"]}],"source":["!conda create -n awq python=3.10 -y\n","!conda init\n","!conda activate awq\n","!pip install --upgrade pip  # enable PEP 660 support\n","!pip install -e ."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":664441,"status":"ok","timestamp":1733672623262,"user":{"displayName":"Bryan Duong","userId":"08362964251007024437"},"user_tz":360},"id":"JvdNFIKQbv8y","outputId":"89e89243-7397-4b1c-f490-c7cac89a3e9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/llm-awq/awq/kernels\n","running install\n","/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n","!!\n","\n","        ********************************************************************************\n","        Please avoid running ``setup.py`` directly.\n","        Instead, use pypa/build, pypa/installer or other\n","        standards-based tools.\n","\n","        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n","        ********************************************************************************\n","\n","!!\n","  self.initialize_options()\n","/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n","!!\n","\n","        ********************************************************************************\n","        Please avoid running ``setup.py`` and ``easy_install``.\n","        Instead, use pypa/build, pypa/installer or other\n","        standards-based tools.\n","\n","        See https://github.com/pypa/setuptools/issues/917 for details.\n","        ********************************************************************************\n","\n","!!\n","  self.initialize_options()\n","running bdist_egg\n","running egg_info\n","creating awq_inference_engine.egg-info\n","writing awq_inference_engine.egg-info/PKG-INFO\n","writing dependency_links to awq_inference_engine.egg-info/dependency_links.txt\n","writing requirements to awq_inference_engine.egg-info/requires.txt\n","writing top-level names to awq_inference_engine.egg-info/top_level.txt\n","writing manifest file 'awq_inference_engine.egg-info/SOURCES.txt'\n","/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:497: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","  warnings.warn(msg.format('we could not find ninja.'))\n","reading manifest file 'awq_inference_engine.egg-info/SOURCES.txt'\n","writing manifest file 'awq_inference_engine.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_ext\n","/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:416: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n","  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:426: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n","  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n","building 'awq_inference_engine' extension\n","creating build/temp.linux-x86_64-cpython-310/csrc/attention\n","creating build/temp.linux-x86_64-cpython-310/csrc/layernorm\n","creating build/temp.linux-x86_64-cpython-310/csrc/position_embedding\n","creating build/temp.linux-x86_64-cpython-310/csrc/quantization\n","creating build/temp.linux-x86_64-cpython-310/csrc/quantization_new/gemm\n","creating build/temp.linux-x86_64-cpython-310/csrc/quantization_new/gemv\n","/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n","If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n","  warnings.warn(\n","/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/attention/decoder_masked_multihead_attention.cu -o build/temp.linux-x86_64-cpython-310/csrc/attention/decoder_masked_multihead_attention.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --threads=8 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=awq_inference_engine -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n","csrc/attention/decoder_masked_multihead_attention_template.hpp(989): warning #177-D: variable \"v_offset\" was declared but never referenced\n","      int v_offset = k_offset;\n","          ^\n","          detected during:\n","            instantiation of \"void mmha_launch_kernel<T,Dh,Dh_MAX,KERNEL_PARAMS_TYPE>(const KERNEL_PARAMS_TYPE &, const cudaStream_t &) [with T=float, Dh=32, Dh_MAX=32, KERNEL_PARAMS_TYPE=Multihead_attention_params<float, false>]\" at line 70 of csrc/attention/decoder_masked_multihead_attention.cu\n","            instantiation of \"void multihead_attention_<T,KERNEL_PARAMS_TYPE>(const KERNEL_PARAMS_TYPE &, const cudaStream_t &) [with T=float, KERNEL_PARAMS_TYPE=Multihead_attention_params<float, false>]\" at line 111 of csrc/attention/decoder_masked_multihead_attention.cu\n","\n","Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","csrc/attention/decoder_masked_multihead_attention_template.hpp(995): warning #177-D: variable \"v_bias_offset\" was declared but never referenced\n","      int v_bias_offset = k_bias_offset;\n","          ^\n","          detected during:\n","            instantiation of \"void mmha_launch_kernel<T,Dh,Dh_MAX,KERNEL_PARAMS_TYPE>(const KERNEL_PARAMS_TYPE &, const cudaStream_t &) [with T=float, Dh=32, Dh_MAX=32, KERNEL_PARAMS_TYPE=Multihead_attention_params<float, false>]\" at line 70 of csrc/attention/decoder_masked_multihead_attention.cu\n","            instantiation of \"void multihead_attention_<T,KERNEL_PARAMS_TYPE>(const KERNEL_PARAMS_TYPE &, const cudaStream_t &) [with T=float, KERNEL_PARAMS_TYPE=Multihead_attention_params<float, false>]\" at line 111 of csrc/attention/decoder_masked_multihead_attention.cu\n","\n","x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/attention/ft_attention.cpp -o build/temp.linux-x86_64-cpython-310/csrc/attention/ft_attention.o -g -O3 -fopenmp -lgomp -std=c++17 -DENABLE_BF16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=awq_inference_engine -D_GLIBCXX_USE_CXX11_ABI=0\n","csrc/attention/ft_attention.cpp: In instantiation of ‘\u001b[01m\u001b[Kvoid set_params(Masked_multihead_attention_params<T>&, size_t, size_t, size_t, size_t, size_t, int, int, float, float, bool, int, T*, T*, T*, T*, T*, int*, float*, T*) [with T = short unsigned int; Masked_multihead_attention_params<T> = Multihead_attention_params<short unsigned int, false>; size_t = long unsigned int]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Kcsrc/attention/ft_attention.cpp:166:5:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[Kcsrc/attention/ft_attention.cpp:73:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* memset(void*, int, size_t)\u001b[m\u001b[K’ clearing an object of non-trivial type ‘\u001b[01m\u001b[KMasked_multihead_attention_params<short unsigned int>\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kstruct Multihead_attention_params<short unsigned int, false>\u001b[m\u001b[K’}; use assignment or value-initialization instead [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wclass-memaccess\u0007-Wclass-memaccess\u001b]8;;\u0007\u001b[m\u001b[K]\n","   73 |     \u001b[01;35m\u001b[Kmemset(&params, 0, sizeof(params))\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","In file included from \u001b[01m\u001b[Kcsrc/attention/ft_attention.cpp:8\u001b[m\u001b[K:\n","\u001b[01m\u001b[Kcsrc/attention/decoder_masked_multihead_attention.h:122:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[KMasked_multihead_attention_params<short unsigned int>\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kstruct Multihead_attention_params<short unsigned int, false>\u001b[m\u001b[K’} declared here\n","  122 | struct \u001b[01;36m\u001b[KMultihead_attention_params\u001b[m\u001b[K: public Multihead_attention_params_base<T> {\n","      |        \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","csrc/attention/ft_attention.cpp: In instantiation of ‘\u001b[01m\u001b[Kvoid set_params(Masked_multihead_attention_params<T>&, size_t, size_t, size_t, size_t, size_t, int, int, float, float, bool, int, T*, T*, T*, T*, T*, int*, float*, T*) [with T = __nv_bfloat16; Masked_multihead_attention_params<T> = Multihead_attention_params<__nv_bfloat16, false>; size_t = long unsigned int]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Kcsrc/attention/ft_attention.cpp:166:5:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[Kcsrc/attention/ft_attention.cpp:73:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* memset(void*, int, size_t)\u001b[m\u001b[K’ clearing an object of non-trivial type ‘\u001b[01m\u001b[KMasked_multihead_attention_params<__nv_bfloat16>\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kstruct Multihead_attention_params<__nv_bfloat16, false>\u001b[m\u001b[K’}; use assignment or value-initialization instead [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wclass-memaccess\u0007-Wclass-memaccess\u001b]8;;\u0007\u001b[m\u001b[K]\n","   73 |     \u001b[01;35m\u001b[Kmemset(&params, 0, sizeof(params))\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","In file included from \u001b[01m\u001b[Kcsrc/attention/ft_attention.cpp:8\u001b[m\u001b[K:\n","\u001b[01m\u001b[Kcsrc/attention/decoder_masked_multihead_attention.h:122:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[KMasked_multihead_attention_params<__nv_bfloat16>\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kstruct Multihead_attention_params<__nv_bfloat16, false>\u001b[m\u001b[K’} declared here\n","  122 | struct \u001b[01;36m\u001b[KMultihead_attention_params\u001b[m\u001b[K: public Multihead_attention_params_base<T> {\n","      |        \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","csrc/attention/ft_attention.cpp: In instantiation of ‘\u001b[01m\u001b[Kvoid set_params(Masked_multihead_attention_params<T>&, size_t, size_t, size_t, size_t, size_t, int, int, float, float, bool, int, T*, T*, T*, T*, T*, int*, float*, T*) [with T = float; Masked_multihead_attention_params<T> = Multihead_attention_params<float, false>; size_t = long unsigned int]\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Kcsrc/attention/ft_attention.cpp:166:5:\u001b[m\u001b[K   required from here\n","\u001b[01m\u001b[Kcsrc/attention/ft_attention.cpp:73:11:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid* memset(void*, int, size_t)\u001b[m\u001b[K’ clearing an object of non-trivial type ‘\u001b[01m\u001b[KMasked_multihead_attention_params<float>\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kstruct Multihead_attention_params<float, false>\u001b[m\u001b[K’}; use assignment or value-initialization instead [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wclass-memaccess\u0007-Wclass-memaccess\u001b]8;;\u0007\u001b[m\u001b[K]\n","   73 |     \u001b[01;35m\u001b[Kmemset(&params, 0, sizeof(params))\u001b[m\u001b[K;\n","      |     \u001b[01;35m\u001b[K~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","In file included from \u001b[01m\u001b[Kcsrc/attention/ft_attention.cpp:8\u001b[m\u001b[K:\n","\u001b[01m\u001b[Kcsrc/attention/decoder_masked_multihead_attention.h:122:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[KMasked_multihead_attention_params<float>\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Kstruct Multihead_attention_params<float, false>\u001b[m\u001b[K’} declared here\n","  122 | struct \u001b[01;36m\u001b[KMultihead_attention_params\u001b[m\u001b[K: public Multihead_attention_params_base<T> {\n","      |        \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/layernorm/layernorm.cu -o build/temp.linux-x86_64-cpython-310/csrc/layernorm/layernorm.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --threads=8 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=awq_inference_engine -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n","/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/position_embedding/pos_encoding_kernels.cu -o build/temp.linux-x86_64-cpython-310/csrc/position_embedding/pos_encoding_kernels.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --threads=8 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=awq_inference_engine -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n","x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/pybind.cpp -o build/temp.linux-x86_64-cpython-310/csrc/pybind.o -g -O3 -fopenmp -lgomp -std=c++17 -DENABLE_BF16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=awq_inference_engine -D_GLIBCXX_USE_CXX11_ABI=0\n","/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/quantization/gemm_cuda_gen.cu -o build/temp.linux-x86_64-cpython-310/csrc/quantization/gemm_cuda_gen.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --threads=8 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=awq_inference_engine -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n","csrc/quantization/gemm_cuda_gen.cu(34): warning #177-D: variable \"ZERO\" was declared but never referenced\n","    static constexpr uint32_t ZERO = 0x0;\n","                              ^\n","\n","Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","csrc/quantization/gemm_cuda_gen.cu(44): warning #177-D: variable \"blockIdx_x\" was declared but never referenced\n","    int blockIdx_x = 0;\n","        ^\n","\n","csrc/quantization/gemm_cuda_gen.cu(65): warning #177-D: variable \"ld_zero_flag\" was declared but never referenced\n","    bool ld_zero_flag = (threadIdx.y * 32 + threadIdx.x) * 8 < 64;\n","         ^\n","\n","csrc/quantization/gemm_cuda_gen.cu(21): warning #177-D: function \"__pack_half2\" was declared but never referenced\n","  __pack_half2(const half x, const half y) {\n","  ^\n","\n","/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/quantization/gemv_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/quantization/gemv_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --threads=8 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=awq_inference_engine -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n","csrc/quantization/gemv_cuda.cu(224): warning #177-D: variable \"blockDim_z\" was declared but never referenced\n","      int blockDim_z = num_out_feats;\n","          ^\n","\n","Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/quantization_new/gemm/gemm_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/quantization_new/gemm/gemm_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --threads=8 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=awq_inference_engine -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n","csrc/quantization_new/gemm/gemm_cuda.cu(300): warning #177-D: variable \"blockIdx_x\" was declared but never referenced\n","    int blockIdx_x = 0;\n","        ^\n","\n","Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","csrc/quantization_new/gemm/gemm_cuda.cu(320): warning #177-D: variable \"kSmemSizeZeros\" was declared but never referenced\n","    constexpr int kSmemSizeZeros = CTA_N * STAGES / scales_load_interval * scales_per_load;\n","                  ^\n","\n","csrc/quantization_new/gemm/gemm_cuda.cu(205): warning #177-D: variable \"total_global_iters\" was declared but never referenced\n","    constexpr int total_global_iters = LD_AMOUNT / 8 / threads_used;\n","                  ^\n","          detected during instantiation of \"void gemm_w4a16_T1<CTA_M,CTA_N,CTA_K,WARP_M,WARP_N,WARP_K,STAGES,G,SPLITK>(half *, half *, half *, half *, half *, int *, int, int, int) [with CTA_M=16, CTA_N=128, CTA_K=128, WARP_M=16, WARP_N=32, WARP_K=64, STAGES=4, G=128, SPLITK=2]\" \n","\n","csrc/quantization_new/gemm/gemm_cuda.cu(753): warning #177-D: variable \"blockIdx_x\" was declared but never referenced\n","    int blockIdx_x = 0;\n","        ^\n","\n","csrc/quantization_new/gemm/gemm_cuda.cu(755): warning #177-D: variable \"blockIdx_z\" was declared but never referenced\n","    int blockIdx_z = blockIdx.x / (num_blocks_m * num_blocks_n);\n","        ^\n","\n","csrc/quantization_new/gemm/gemm_cuda.cu(771): warning #177-D: variable \"kSmemSizeZeros\" was declared but never referenced\n","    constexpr int kSmemSizeZeros = CTA_N * STAGES / 2;\n","                  ^\n","\n","csrc/quantization_new/gemm/gemm_cuda.cu(662): warning #177-D: variable \"total_global_iters\" was declared but never referenced\n","    constexpr int total_global_iters = CTA_N / 8 / threads_used;\n","                  ^\n","          detected during instantiation of \"void gemm_w4a16_T2<CTA_M,CTA_N,CTA_K,WARP_M,WARP_N,WARP_K,STAGES,G>(half *, half *, half *, half *, half *, int, int, int) [with CTA_M=64, CTA_N=128, CTA_K=64, WARP_M=64, WARP_N=32, WARP_K=64, STAGES=4, G=128]\" \n","\n","csrc/quantization_new/gemm/gemm_cuda.cu(664): warning #177-D: variable \"kSmemCol\" was declared but never referenced\n","    constexpr int kSmemCol = CTA_N;\n","                  ^\n","          detected during instantiation of \"void gemm_w4a16_T2<CTA_M,CTA_N,CTA_K,WARP_M,WARP_N,WARP_K,STAGES,G>(half *, half *, half *, half *, half *, int, int, int) [with CTA_M=64, CTA_N=128, CTA_K=64, WARP_M=64, WARP_N=32, WARP_K=64, STAGES=4, G=128]\" \n","\n","/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/quantization_new/gemv/gemv_cuda.cu -o build/temp.linux-x86_64-cpython-310/csrc/quantization_new/gemv/gemv_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -DENABLE_BF16 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math --threads=8 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=awq_inference_engine -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80\n","csrc/quantization_new/gemv/gemv_cuda.cu(83): warning #177-D: variable \"kShuffleSize\" was declared but never referenced\n","      static constexpr int kShuffleSize = 32;\n","                           ^\n","\n","Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n","\n","creating build/lib.linux-x86_64-cpython-310\n","x86_64-linux-gnu-g++ -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions build/temp.linux-x86_64-cpython-310/csrc/attention/decoder_masked_multihead_attention.o build/temp.linux-x86_64-cpython-310/csrc/attention/ft_attention.o build/temp.linux-x86_64-cpython-310/csrc/layernorm/layernorm.o build/temp.linux-x86_64-cpython-310/csrc/position_embedding/pos_encoding_kernels.o build/temp.linux-x86_64-cpython-310/csrc/pybind.o build/temp.linux-x86_64-cpython-310/csrc/quantization/gemm_cuda_gen.o build/temp.linux-x86_64-cpython-310/csrc/quantization/gemv_cuda.o build/temp.linux-x86_64-cpython-310/csrc/quantization_new/gemm/gemm_cuda.o build/temp.linux-x86_64-cpython-310/csrc/quantization_new/gemv/gemv_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/awq_inference_engine.cpython-310-x86_64-linux-gnu.so\n","creating build/bdist.linux-x86_64/egg\n","copying build/lib.linux-x86_64-cpython-310/awq_inference_engine.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n","creating stub loader for awq_inference_engine.cpython-310-x86_64-linux-gnu.so\n","byte-compiling build/bdist.linux-x86_64/egg/awq_inference_engine.py to awq_inference_engine.cpython-310.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying awq_inference_engine.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying awq_inference_engine.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying awq_inference_engine.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying awq_inference_engine.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying awq_inference_engine.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n","zip_safe flag not set; analyzing archive contents...\n","__pycache__.awq_inference_engine.cpython-310: module references __file__\n","creating dist\n","creating 'dist/awq_inference_engine-0.0.0-py3.10-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing awq_inference_engine-0.0.0-py3.10-linux-x86_64.egg\n","creating /usr/local/lib/python3.10/dist-packages/awq_inference_engine-0.0.0-py3.10-linux-x86_64.egg\n","Extracting awq_inference_engine-0.0.0-py3.10-linux-x86_64.egg to /usr/local/lib/python3.10/dist-packages\n","Adding awq-inference-engine 0.0.0 to easy-install.pth file\n","\n","Installed /usr/local/lib/python3.10/dist-packages/awq_inference_engine-0.0.0-py3.10-linux-x86_64.egg\n","Processing dependencies for awq-inference-engine==0.0.0\n","Searching for torch==2.5.1+cu121\n","Best match: torch 2.5.1+cu121\n","Adding torch 2.5.1+cu121 to easy-install.pth file\n","detected new path './awq_inference_engine-0.0.0-py3.10-linux-x86_64.egg'\n","Installing convert-caffe2-to-onnx script to /usr/local/bin\n","Installing convert-onnx-to-caffe2 script to /usr/local/bin\n","Installing torchfrtrace script to /usr/local/bin\n","Installing torchrun script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for sympy==1.13.1\n","Best match: sympy 1.13.1\n","Adding sympy 1.13.1 to easy-install.pth file\n","Installing isympy script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for fsspec==2024.9.0\n","Best match: fsspec 2024.9.0\n","Adding fsspec 2024.9.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for jinja2==3.1.4\n","Best match: jinja2 3.1.4\n","Adding jinja2 3.1.4 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for networkx==3.4.2\n","Best match: networkx 3.4.2\n","Adding networkx 3.4.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for typing-extensions==4.12.2\n","Best match: typing-extensions 4.12.2\n","Adding typing-extensions 4.12.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages/setuptools/_vendor\n","Searching for filelock==3.16.1\n","Best match: filelock 3.16.1\n","Adding filelock 3.16.1 to easy-install.pth file\n","detected new path './setuptools/_vendor'\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for mpmath==1.3.0\n","Best match: mpmath 1.3.0\n","Adding mpmath 1.3.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Searching for MarkupSafe==3.0.2\n","Best match: MarkupSafe 3.0.2\n","Adding MarkupSafe 3.0.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.10/dist-packages\n","Finished processing dependencies for awq-inference-engine==0.0.0\n"]}],"source":["%cd awq/kernels\n","!python setup.py install"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97,"status":"ok","timestamp":1733672633393,"user":{"displayName":"Bryan Duong","userId":"08362964251007024437"},"user_tz":360},"id":"scecQK9zKI3V","outputId":"177d4fb1-1ea5-4b04-df21-c558d0ccb4f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/llm-awq/awq\n","/content/llm-awq\n"]}],"source":["%cd ..\n","%cd .."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3VGOnXfespA","outputId":"938cdd77-6346-48cf-dea4-1999e362d501","executionInfo":{"status":"ok","timestamp":1733674203756,"user_tz":360,"elapsed":59530,"user":{"displayName":"Bryan Duong","userId":"08362964251007024437"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Enter your token (input will not be visible): \n","Add token as git credential? (Y/n) y\n","Token is valid (permission: fineGrained).\n","The token `bryand_token` has been saved to /root/.cache/huggingface/stored_tokens\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `bryand_token`\n","Fetching 1 files:   0% 0/1 [00:00<?, ?it/s]Downloading 'pytorch_model-00001-of-00003.bin' to 'facebook/opt-13b/.cache/huggingface/download/pytorch_model-00001-of-00003.bin.7f92dff6b6debf90135157bfe3663ea87f5dc17fef486e8981259692df3e284c.incomplete'\n","\n","pytorch_model-00001-of-00003.bin:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   0% 10.5M/9.98G [00:00<01:47, 92.4MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   0% 41.9M/9.98G [00:00<00:52, 190MB/s] \u001b[A\n","pytorch_model-00001-of-00003.bin:   1% 62.9M/9.98G [00:00<00:51, 191MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   1% 94.4M/9.98G [00:00<00:46, 212MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   1% 126M/9.98G [00:00<00:44, 220MB/s] \u001b[A\n","pytorch_model-00001-of-00003.bin:   2% 157M/9.98G [00:00<00:43, 227MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   2% 189M/9.98G [00:00<00:42, 229MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   2% 220M/9.98G [00:01<00:42, 231MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   3% 252M/9.98G [00:01<00:41, 236MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   3% 283M/9.98G [00:01<00:40, 240MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   3% 315M/9.98G [00:01<00:41, 235MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   3% 346M/9.98G [00:01<00:41, 233MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   4% 377M/9.98G [00:01<00:40, 237MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   4% 409M/9.98G [00:01<00:39, 241MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   4% 440M/9.98G [00:01<00:39, 241MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   5% 472M/9.98G [00:02<00:39, 238MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   5% 503M/9.98G [00:02<00:39, 238MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   5% 535M/9.98G [00:02<00:40, 231MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   6% 566M/9.98G [00:02<00:41, 228MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   6% 598M/9.98G [00:02<00:40, 233MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   6% 629M/9.98G [00:02<00:40, 230MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   7% 661M/9.98G [00:02<00:39, 235MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   7% 692M/9.98G [00:03<00:39, 233MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   7% 724M/9.98G [00:03<00:39, 235MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   8% 755M/9.98G [00:03<00:38, 237MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   8% 786M/9.98G [00:03<00:47, 195MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   8% 807M/9.98G [00:03<00:47, 193MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   8% 839M/9.98G [00:03<00:45, 201MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   9% 870M/9.98G [00:03<00:42, 212MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   9% 902M/9.98G [00:04<00:40, 223MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:   9% 933M/9.98G [00:04<00:39, 231MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  10% 965M/9.98G [00:04<00:38, 237MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  10% 996M/9.98G [00:04<00:37, 239MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  10% 1.03G/9.98G [00:04<00:37, 240MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  11% 1.06G/9.98G [00:04<00:37, 239MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  11% 1.09G/9.98G [00:04<00:36, 241MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  11% 1.12G/9.98G [00:04<00:36, 241MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  12% 1.15G/9.98G [00:05<00:36, 239MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  12% 1.18G/9.98G [00:05<00:36, 240MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  12% 1.22G/9.98G [00:05<00:36, 238MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  13% 1.25G/9.98G [00:05<00:36, 237MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  13% 1.28G/9.98G [00:05<00:42, 206MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  13% 1.31G/9.98G [00:05<00:40, 215MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  13% 1.34G/9.98G [00:05<00:38, 222MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  14% 1.37G/9.98G [00:06<00:38, 226MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  14% 1.41G/9.98G [00:06<00:36, 232MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  14% 1.44G/9.98G [00:06<00:36, 236MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  15% 1.47G/9.98G [00:06<00:46, 182MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  15% 1.50G/9.98G [00:06<00:43, 195MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  15% 1.53G/9.98G [00:06<00:40, 208MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  16% 1.56G/9.98G [00:06<00:38, 221MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  16% 1.59G/9.98G [00:07<00:37, 226MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  16% 1.63G/9.98G [00:07<00:36, 232MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  17% 1.66G/9.98G [00:07<00:35, 233MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  17% 1.69G/9.98G [00:07<00:45, 181MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  17% 1.72G/9.98G [00:07<00:42, 194MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  18% 1.75G/9.98G [00:07<00:39, 207MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  18% 1.78G/9.98G [00:07<00:37, 217MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  18% 1.81G/9.98G [00:08<00:36, 225MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  19% 1.85G/9.98G [00:08<00:35, 230MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  19% 1.88G/9.98G [00:08<00:34, 232MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  19% 1.91G/9.98G [00:08<00:44, 181MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  19% 1.94G/9.98G [00:08<00:40, 197MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  20% 1.97G/9.98G [00:08<00:38, 209MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  20% 2.00G/9.98G [00:09<00:36, 217MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  20% 2.03G/9.98G [00:09<00:36, 219MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  21% 2.07G/9.98G [00:09<00:35, 220MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  21% 2.10G/9.98G [00:09<00:43, 181MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  21% 2.13G/9.98G [00:09<00:39, 197MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  22% 2.16G/9.98G [00:09<00:37, 210MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  22% 2.19G/9.98G [00:09<00:35, 218MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  22% 2.22G/9.98G [00:10<00:35, 220MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  23% 2.25G/9.98G [00:10<00:33, 231MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  23% 2.29G/9.98G [00:10<00:33, 233MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  23% 2.32G/9.98G [00:10<00:42, 181MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  24% 2.35G/9.98G [00:10<00:39, 192MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  24% 2.38G/9.98G [00:10<00:37, 202MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  24% 2.41G/9.98G [00:11<00:36, 210MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  24% 2.44G/9.98G [00:11<00:34, 218MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  25% 2.47G/9.98G [00:11<00:33, 224MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  25% 2.51G/9.98G [00:11<00:32, 229MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  25% 2.54G/9.98G [00:11<00:39, 186MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  26% 2.57G/9.98G [00:11<00:37, 199MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  26% 2.60G/9.98G [00:11<00:34, 212MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  26% 2.63G/9.98G [00:12<00:33, 222MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  27% 2.66G/9.98G [00:12<00:32, 223MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  27% 2.69G/9.98G [00:12<00:31, 231MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  27% 2.73G/9.98G [00:12<00:39, 182MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  28% 2.76G/9.98G [00:12<00:36, 197MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  28% 2.79G/9.98G [00:12<00:34, 207MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  28% 2.82G/9.98G [00:12<00:33, 215MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  29% 2.85G/9.98G [00:13<00:32, 219MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  29% 2.88G/9.98G [00:13<00:31, 228MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  29% 2.92G/9.98G [00:13<00:45, 155MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  30% 2.95G/9.98G [00:13<00:39, 176MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  30% 2.98G/9.98G [00:13<00:36, 193MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  30% 3.01G/9.98G [00:13<00:33, 208MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  30% 3.04G/9.98G [00:14<00:31, 218MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  31% 3.07G/9.98G [00:14<00:30, 226MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  31% 3.10G/9.98G [00:14<00:30, 222MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  31% 3.14G/9.98G [00:14<00:30, 227MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  32% 3.17G/9.98G [00:14<00:31, 214MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  32% 3.20G/9.98G [00:14<00:30, 220MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  32% 3.23G/9.98G [00:14<00:29, 226MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  33% 3.26G/9.98G [00:15<00:29, 225MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  33% 3.29G/9.98G [00:15<00:28, 232MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  33% 3.32G/9.98G [00:15<00:27, 238MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  34% 3.36G/9.98G [00:15<00:35, 184MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  34% 3.39G/9.98G [00:15<00:33, 198MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  34% 3.42G/9.98G [00:15<00:31, 208MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  35% 3.45G/9.98G [00:15<00:30, 214MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  35% 3.48G/9.98G [00:16<00:29, 217MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  35% 3.51G/9.98G [00:16<00:28, 223MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  36% 3.54G/9.98G [00:16<00:28, 227MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  36% 3.58G/9.98G [00:16<00:33, 188MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  36% 3.61G/9.98G [00:16<00:31, 203MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  36% 3.64G/9.98G [00:16<00:29, 212MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  37% 3.67G/9.98G [00:16<00:28, 219MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  37% 3.70G/9.98G [00:17<00:28, 223MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  37% 3.73G/9.98G [00:17<00:27, 230MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  38% 3.76G/9.98G [00:17<00:26, 234MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  38% 3.80G/9.98G [00:17<00:34, 181MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  38% 3.83G/9.98G [00:17<00:31, 196MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  39% 3.86G/9.98G [00:17<00:29, 207MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  39% 3.89G/9.98G [00:18<00:28, 215MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  39% 3.92G/9.98G [00:18<00:27, 221MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  40% 3.95G/9.98G [00:18<00:28, 214MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  40% 3.98G/9.98G [00:18<00:32, 184MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  40% 4.02G/9.98G [00:18<00:30, 197MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  41% 4.05G/9.98G [00:18<00:28, 207MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  41% 4.08G/9.98G [00:18<00:27, 214MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  41% 4.11G/9.98G [00:19<00:27, 217MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  42% 4.14G/9.98G [00:19<00:29, 199MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  42% 4.16G/9.98G [00:19<00:29, 196MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  42% 4.18G/9.98G [00:19<00:29, 195MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  42% 4.22G/9.98G [00:19<00:28, 204MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  42% 4.24G/9.98G [00:19<00:28, 203MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  43% 4.27G/9.98G [00:19<00:26, 214MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  43% 4.30G/9.98G [00:20<00:25, 226MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  43% 4.33G/9.98G [00:20<00:24, 233MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  44% 4.36G/9.98G [00:20<00:23, 243MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  44% 4.39G/9.98G [00:20<00:22, 245MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  44% 4.42G/9.98G [00:20<00:29, 187MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  45% 4.46G/9.98G [00:20<00:27, 201MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  45% 4.49G/9.98G [00:20<00:26, 207MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  45% 4.52G/9.98G [00:21<00:26, 208MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  46% 4.55G/9.98G [00:21<00:24, 219MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  46% 4.58G/9.98G [00:21<00:23, 233MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  46% 4.61G/9.98G [00:21<00:29, 184MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  47% 4.65G/9.98G [00:21<00:26, 198MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  47% 4.68G/9.98G [00:21<00:25, 210MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  47% 4.71G/9.98G [00:21<00:24, 219MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  48% 4.74G/9.98G [00:22<00:23, 222MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  48% 4.77G/9.98G [00:22<00:22, 228MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  48% 4.80G/9.98G [00:22<00:22, 233MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  48% 4.83G/9.98G [00:22<00:28, 179MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  49% 4.87G/9.98G [00:22<00:26, 191MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  49% 4.90G/9.98G [00:22<00:25, 202MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  49% 4.93G/9.98G [00:23<00:23, 215MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  50% 4.96G/9.98G [00:23<00:22, 228MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  50% 4.99G/9.98G [00:23<00:21, 235MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  50% 5.02G/9.98G [00:23<00:21, 235MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  51% 5.05G/9.98G [00:23<00:27, 180MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  51% 5.09G/9.98G [00:23<00:25, 193MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  51% 5.12G/9.98G [00:23<00:24, 201MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  52% 5.15G/9.98G [00:24<00:22, 213MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  52% 5.18G/9.98G [00:24<00:21, 224MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  52% 5.21G/9.98G [00:24<00:20, 232MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  53% 5.24G/9.98G [00:24<00:25, 186MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  53% 5.27G/9.98G [00:24<00:23, 201MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  53% 5.31G/9.98G [00:24<00:22, 210MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  54% 5.34G/9.98G [00:25<00:31, 148MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  54% 5.37G/9.98G [00:25<00:27, 166MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  54% 5.40G/9.98G [00:25<00:24, 184MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  54% 5.43G/9.98G [00:25<00:22, 199MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  55% 5.46G/9.98G [00:25<00:21, 207MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  55% 5.49G/9.98G [00:25<00:20, 223MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  55% 5.53G/9.98G [00:25<00:18, 235MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  56% 5.56G/9.98G [00:26<00:18, 236MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  56% 5.59G/9.98G [00:26<00:17, 244MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  56% 5.62G/9.98G [00:26<00:17, 242MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  57% 5.65G/9.98G [00:26<00:17, 242MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  57% 5.68G/9.98G [00:26<00:20, 213MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  57% 5.71G/9.98G [00:26<00:19, 221MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  58% 5.75G/9.98G [00:26<00:18, 228MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  58% 5.78G/9.98G [00:27<00:18, 223MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  58% 5.81G/9.98G [00:27<00:18, 227MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  59% 5.84G/9.98G [00:27<00:17, 235MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  59% 5.87G/9.98G [00:27<00:22, 185MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  59% 5.90G/9.98G [00:27<00:20, 197MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  59% 5.93G/9.98G [00:27<00:19, 205MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  60% 5.97G/9.98G [00:27<00:18, 215MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  60% 6.00G/9.98G [00:28<00:17, 223MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  60% 6.03G/9.98G [00:28<00:17, 228MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  61% 6.06G/9.98G [00:28<00:16, 232MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  61% 6.09G/9.98G [00:28<00:21, 185MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  61% 6.12G/9.98G [00:28<00:19, 200MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  62% 6.16G/9.98G [00:28<00:18, 206MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  62% 6.19G/9.98G [00:28<00:17, 219MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  62% 6.22G/9.98G [00:29<00:16, 229MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  63% 6.25G/9.98G [00:29<00:15, 237MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  63% 6.28G/9.98G [00:29<00:15, 236MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  63% 6.31G/9.98G [00:29<00:20, 180MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  64% 6.34G/9.98G [00:29<00:18, 195MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  64% 6.38G/9.98G [00:29<00:17, 207MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  64% 6.41G/9.98G [00:30<00:16, 215MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  65% 6.44G/9.98G [00:30<00:16, 213MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  65% 6.47G/9.98G [00:30<00:16, 218MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  65% 6.50G/9.98G [00:30<00:18, 184MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  65% 6.53G/9.98G [00:30<00:17, 198MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  66% 6.56G/9.98G [00:30<00:16, 203MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  66% 6.60G/9.98G [00:30<00:15, 212MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  66% 6.63G/9.98G [00:31<00:15, 218MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  67% 6.66G/9.98G [00:31<00:14, 228MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  67% 6.69G/9.98G [00:31<00:14, 226MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  67% 6.72G/9.98G [00:31<00:17, 187MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  68% 6.75G/9.98G [00:31<00:16, 200MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  68% 6.78G/9.98G [00:31<00:15, 211MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  68% 6.82G/9.98G [00:32<00:14, 216MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  69% 6.85G/9.98G [00:32<00:14, 222MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  69% 6.88G/9.98G [00:32<00:13, 230MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  69% 6.91G/9.98G [00:32<00:13, 230MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  70% 6.94G/9.98G [00:32<00:16, 183MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  70% 6.97G/9.98G [00:32<00:15, 197MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  70% 7.00G/9.98G [00:32<00:14, 208MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  71% 7.04G/9.98G [00:33<00:13, 218MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  71% 7.07G/9.98G [00:33<00:13, 223MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  71% 7.10G/9.98G [00:33<00:12, 225MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  71% 7.13G/9.98G [00:33<00:15, 183MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  72% 7.16G/9.98G [00:33<00:14, 197MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  72% 7.19G/9.98G [00:33<00:13, 204MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  72% 7.22G/9.98G [00:34<00:14, 196MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  73% 7.26G/9.98G [00:34<00:12, 211MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  73% 7.29G/9.98G [00:34<00:12, 216MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  73% 7.32G/9.98G [00:34<00:12, 218MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  74% 7.35G/9.98G [00:34<00:13, 192MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  74% 7.38G/9.98G [00:34<00:12, 201MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  74% 7.41G/9.98G [00:34<00:11, 216MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  75% 7.44G/9.98G [00:35<00:11, 222MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  75% 7.48G/9.98G [00:35<00:12, 200MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  75% 7.50G/9.98G [00:35<00:12, 198MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  75% 7.52G/9.98G [00:35<00:18, 131MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  76% 7.54G/9.98G [00:35<00:16, 144MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  76% 7.57G/9.98G [00:35<00:14, 165MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  76% 7.60G/9.98G [00:36<00:12, 188MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  77% 7.63G/9.98G [00:36<00:11, 204MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  77% 7.67G/9.98G [00:36<00:11, 209MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  77% 7.70G/9.98G [00:36<00:10, 222MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  77% 7.73G/9.98G [00:36<00:09, 229MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  78% 7.76G/9.98G [00:36<00:09, 234MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  78% 7.79G/9.98G [00:36<00:09, 236MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  78% 7.82G/9.98G [00:36<00:08, 245MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  79% 7.85G/9.98G [00:37<00:08, 249MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  79% 7.89G/9.98G [00:37<00:08, 247MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  79% 7.92G/9.98G [00:37<00:08, 241MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  80% 7.95G/9.98G [00:37<00:08, 239MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  80% 7.98G/9.98G [00:37<00:09, 217MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  80% 8.01G/9.98G [00:37<00:08, 226MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  81% 8.04G/9.98G [00:37<00:08, 225MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  81% 8.07G/9.98G [00:38<00:08, 232MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  81% 8.11G/9.98G [00:38<00:08, 226MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  82% 8.14G/9.98G [00:38<00:07, 233MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  82% 8.17G/9.98G [00:38<00:07, 230MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  82% 8.20G/9.98G [00:38<00:09, 190MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  83% 8.23G/9.98G [00:38<00:09, 193MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  83% 8.26G/9.98G [00:38<00:08, 206MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  83% 8.29G/9.98G [00:39<00:07, 222MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  83% 8.33G/9.98G [00:39<00:07, 233MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  84% 8.36G/9.98G [00:39<00:07, 225MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  84% 8.39G/9.98G [00:39<00:08, 185MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  84% 8.42G/9.98G [00:39<00:07, 197MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  85% 8.45G/9.98G [00:39<00:07, 211MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  85% 8.48G/9.98G [00:39<00:06, 218MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  85% 8.51G/9.98G [00:40<00:06, 224MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  86% 8.55G/9.98G [00:40<00:06, 227MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  86% 8.58G/9.98G [00:40<00:06, 227MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  86% 8.61G/9.98G [00:40<00:07, 178MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  87% 8.63G/9.98G [00:40<00:07, 183MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  87% 8.66G/9.98G [00:40<00:06, 194MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  87% 8.69G/9.98G [00:41<00:06, 204MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  87% 8.72G/9.98G [00:41<00:05, 216MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  88% 8.76G/9.98G [00:41<00:05, 222MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  88% 8.79G/9.98G [00:41<00:05, 232MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  88% 8.82G/9.98G [00:41<00:05, 195MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  89% 8.85G/9.98G [00:41<00:05, 210MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  89% 8.88G/9.98G [00:41<00:05, 211MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  89% 8.91G/9.98G [00:42<00:05, 209MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  90% 8.94G/9.98G [00:42<00:04, 215MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  90% 8.98G/9.98G [00:42<00:04, 215MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  90% 9.01G/9.98G [00:42<00:04, 213MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  91% 9.04G/9.98G [00:42<00:04, 195MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  91% 9.07G/9.98G [00:42<00:04, 205MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  91% 9.10G/9.98G [00:42<00:04, 215MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  92% 9.13G/9.98G [00:43<00:03, 218MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  92% 9.16G/9.98G [00:43<00:03, 222MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  92% 9.20G/9.98G [00:43<00:03, 223MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  93% 9.23G/9.98G [00:43<00:03, 189MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  93% 9.26G/9.98G [00:43<00:03, 202MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  93% 9.29G/9.98G [00:43<00:03, 214MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  93% 9.32G/9.98G [00:43<00:03, 217MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  94% 9.35G/9.98G [00:44<00:02, 223MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  94% 9.38G/9.98G [00:44<00:02, 232MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  94% 9.42G/9.98G [00:44<00:02, 233MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  95% 9.45G/9.98G [00:44<00:02, 183MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  95% 9.48G/9.98G [00:44<00:02, 199MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  95% 9.51G/9.98G [00:44<00:02, 207MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  96% 9.54G/9.98G [00:45<00:02, 215MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  96% 9.57G/9.98G [00:45<00:01, 216MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  96% 9.60G/9.98G [00:45<00:01, 220MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  97% 9.64G/9.98G [00:45<00:01, 212MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  97% 9.67G/9.98G [00:45<00:01, 187MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  97% 9.70G/9.98G [00:45<00:01, 201MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  98% 9.73G/9.98G [00:45<00:01, 209MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  98% 9.76G/9.98G [00:46<00:00, 215MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  98% 9.79G/9.98G [00:46<00:00, 224MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  98% 9.83G/9.98G [00:46<00:00, 225MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  99% 9.86G/9.98G [00:46<00:00, 186MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  99% 9.89G/9.98G [00:46<00:00, 198MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin:  99% 9.92G/9.98G [00:46<00:00, 209MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin: 100% 9.95G/9.98G [00:46<00:00, 217MB/s]\u001b[A\n","pytorch_model-00001-of-00003.bin: 100% 9.98G/9.98G [00:47<00:00, 212MB/s]\n","Download complete. Moving file to facebook/opt-13b/pytorch_model-00001-of-00003.bin\n","Fetching 1 files: 100% 1/1 [00:47<00:00, 47.30s/it]\n","/content/facebook/opt-13b\n"]}],"source":["!huggingface-cli login\n","# !huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --exclude \"original/*\" --local-dir meta-llama/Meta-Llama-3-8B-Instruct\n","\n","# !huggingface-cli download facebook/opt-13b --local-dir facebook/opt-13b --exclude \"*.h5\" --exclude \"*.msgpack\"\n","!huggingface-cli download facebook/opt-13b --local-dir facebook/opt-13b --include \"pytorch_model-00001-of-00003.bin\"\n","\n","\n","# !huggingface-cli download facebook/opt-6.7b --exclude \"tf_model* flax_model*\" --local-dir facebook/opt-6.7b\n","\n","# !huggingface-cli download facebook/opt-1.3b --local-dir facebook/opt-1.3b\n","\n","# !huggingface-cli download meta-llama/Llama-2-7b-chat-hf --exclude \"tf_model* flax_model*\" --local-dir meta-llama/Llama-2-7b-chat\n","# !huggingface-cli download Efficient-Large-Model/VILA-13b --exclude \"tf_model* flax_model*\" --local-dir Efficient-Large-Model/VILA-13b"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1056105,"status":"ok","timestamp":1733675339883,"user":{"displayName":"Bryan Duong","userId":"08362964251007024437"},"user_tz":360},"id":"bx-d4k-okchw","outputId":"01262c90-a9d6-429c-8ed6-3e2173ef8ec5"},"outputs":[{"output_type":"stream","name":"stdout","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n","2024-12-08 16:11:29.077266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-12-08 16:11:29.098261: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-12-08 16:11:29.104635: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-12-08 16:11:30.151067: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Quantization config: {'zero_point': True, 'q_group_size': 128}\n","* Building model /content/llm-awq/facebook/opt-13b/\n","Loading checkpoint shards: 100% 3/3 [00:00<00:00,  3.73it/s]\n","README.md: 100% 167/167 [00:00<00:00, 1.07MB/s]\n","Repo card metadata block was not found. Setting CardData to empty.\n","val.jsonl.zst: 100% 471M/471M [00:02<00:00, 233MB/s]\n","Generating validation split: 100% 214670/214670 [00:15<00:00, 13688.23 examples/s]\n"," * Split into 60 blocks\n","Running AWQ...: 100% 40/40 [16:55<00:00, 25.39s/it]\n","AWQ results saved at awq_cache/opt-13b-w4-g128.pt\n"]}],"source":["# Perform AWQ search and save search results:\n","# !python -m awq.entry --model_path /content/llm-awq/meta-llama/Meta-Llama-3-8B-Instruct/ \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --run_awq --dump_awq awq_cache_bryan/llama3-8b-w4-g128.pt\n","\n","!python -m awq.entry --model_path /content/llm-awq/facebook/opt-13b/ \\\n","    --w_bit 4 --q_group_size 128 \\\n","    --run_awq --dump_awq awq_cache/opt-13b-w4-g128.pt\n","\n","# !python -m awq.entry --model_path /content/llm-awq/facebook/opt-6.7b/ \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --run_awq --dump_awq awq_cache/opt-6.7b-w4-g128.pt\n","\n","# !python -m awq.entry --model_path /content/llm-awq/facebook/opt-1.3b/ \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --run_awq --dump_awq awq_cache/opt-1.3b-w4-g128.pt\n","\n","\n","# !python -m awq.entry --model_path /content/llm-awq/meta-llama/Llama-2-7b-chat/ \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --run_awq --dump_awq awq_cache/Llama-2-7b-chat-w4-g128.pt\n","\n","# !python -m awq.entry --model_path /content/llm-awq/Efficient-Large-Model/VILA-13b/ \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --run_awq --dump_awq awq_cache/VILA-13b-w4-g128.pt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mAnkOK2r3tW"},"outputs":[],"source":["# # Evaluate the AWQ quantized model on WikiText-2 (simulated pseudo quantization)\n","# !python -m awq.entry --model_path /content/llm-awq/meta-llama/Meta-Llama-3-8B-Instruct/ \\\n","#     --tasks wikitext \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --load_awq awq_cache_bryan/llama3-8b-w4-g128.pt \\\n","#     --q_backend fake"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":517158,"status":"ok","timestamp":1733675901989,"user":{"displayName":"Bryan Duong","userId":"08362964251007024437"},"user_tz":360},"id":"zlVX9kFWsn7r","outputId":"a1c803ad-cb9d-4244-867a-84cf5f6a4a16"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-12-08 16:29:49.144909: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-12-08 16:29:49.166019: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-12-08 16:29:49.172402: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-12-08 16:29:50.354424: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Quantization config: {'zero_point': True, 'q_group_size': 128}\n","* Building model /content/llm-awq/facebook/opt-13b/\n","Loading checkpoint shards: 100% 3/3 [00:00<00:00,  3.75it/s]\n","Loading pre-computed AWQ results from awq_cache/opt-13b-w4-g128.pt\n","/content/llm-awq/awq/entry.py:189: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  awq_results = torch.load(args.load_awq, map_location=\"cpu\")\n","real weight quantization...: 100% 40/40 [07:02<00:00, 10.57s/it]\n","[Info] Auto-change the dump_quant file name to *v2.pt\n","Saving the quantized model at quant_cache/opt-13b-w4-g128-awq-v2.pt...\n"]}],"source":["# Generate real quantized weights (INT4)\n","!mkdir quant_cache\n","\n","# !python -m awq.entry --model_path /content/llm-awq/meta-llama/Meta-Llama-3-8B-Instruct/\\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --load_awq awq_cache/llama3-8b-w4-g128.pt \\\n","#     --q_backend real --dump_quant quant_cache/llama3-8b-w4-g128-awq.pt\n","\n","!python -m awq.entry --model_path /content/llm-awq/facebook/opt-13b/ \\\n","    --w_bit 4 --q_group_size 128 \\\n","    --load_awq awq_cache/opt-13b-w4-g128.pt \\\n","    --q_backend real --dump_quant quant_cache/opt-13b-w4-g128-awq.pt\n","\n","# !python -m awq.entry --model_path /content/llm-awq/facebook/opt-6.7b/ \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --load_awq awq_cache/opt-6.7b-w4-g128.pt \\\n","#     --q_backend real --dump_quant quant_cache/opt-6.7b-w4-g128-awq.pt\n","\n","# !python -m awq.entry --model_path /content/llm-awq/facebook/opt-1.3b/ \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --load_awq awq_cache/opt-1.3b-w4-g128.pt \\\n","#     --q_backend real --dump_quant quant_cache/opt-1.3b-w4-g128-awq.pt\n","\n","\n","# !python -m awq.entry --model_path /content/llm-awq/meta-llama/Llama-2-7b-chat/ \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --load_awq awq_cache/Llama-2-7b-chat-w4-g128.pt \\\n","#     --q_backend real --dump_quant quant_cache/Llama-2-7b-chat-w4-g128-awq.pt\n","\n","# !python -m awq.entry --model_path /content/llm-awq/Efficient-Large-Model/VILA-13b/ \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --load_awq awq_cache/VILA-13b-w4-g128.pt \\\n","#     --q_backend real --dump_quant quant_cache/VILA-13b-w4-g128-awq.pt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":104291,"status":"ok","timestamp":1733676290541,"user":{"displayName":"Bryan Duong","userId":"08362964251007024437"},"user_tz":360},"id":"BWWN_xMht6nW","outputId":"818f7189-b12c-475d-f52e-15888178aeb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-12-08 16:43:09.932500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-12-08 16:43:09.953521: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-12-08 16:43:09.959954: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-12-08 16:43:11.145438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Quantization config: {'zero_point': True, 'q_group_size': 128}\n","* Building model /content/llm-awq/facebook/opt-13b/\n","Loading pre-computed quantized weights...\n","real weight quantization...(init only): 100% 40/40 [00:00<00:00, 859.77it/s]\n","/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:1513: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(checkpoint_file, map_location=torch.device(\"cpu\"))\n","README.md: 100% 10.5k/10.5k [00:00<00:00, 48.5MB/s]\n","test-00000-of-00001.parquet: 100% 733k/733k [00:00<00:00, 12.9MB/s]\n","train-00000-of-00001.parquet: 100% 6.36M/6.36M [00:00<00:00, 140MB/s]\n","validation-00000-of-00001.parquet: 100% 657k/657k [00:00<00:00, 410MB/s]\n","Generating test split: 100% 4358/4358 [00:00<00:00, 160083.17 examples/s]\n","Generating train split: 100% 36718/36718 [00:00<00:00, 678744.53 examples/s]\n","Generating validation split: 100% 3760/3760 [00:00<00:00, 571542.89 examples/s]\n","evaluating...: 100% 140/140 [01:22<00:00,  1.70it/s]\n","10.293193817138672\n"]}],"source":["# Load and evaluate the real quantized model (now you can see smaller gpu memory usage)\n","# !python -m awq.entry --model_path /content/llm-awq/meta-llama/Meta-Llama-3-8B-Instruct/ \\\n","#     --tasks wikitext \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --load_quant quant_cache/llama3-8b-w4-g128-awq.pt\n","\n","!python -m awq.entry --model_path /content/llm-awq/facebook/opt-13b/ \\\n","    --tasks wikitext \\\n","    --w_bit 4 --q_group_size 128 \\\n","    --load_quant quant_cache/opt-13b-w4-g128-awq-v2.pt\n","\n","# !python -m awq.entry --model_path /content/llm-awq/facebook/opt-6.7b/ \\\n","#     --tasks wikitext \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --load_quant quant_cache/opt-6.7b-w4-g128-awq-v2.pt\n","\n","# !python -m awq.entry --model_path /content/llm-awq/facebook/opt-1.3b/ \\\n","#     --tasks wikitext \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --load_quant quant_cache/opt-1.3b-w4-g128-awq-v2.pt\n","\n","\n","# !python -m awq.entry --model_path /content/llm-awq/meta-llama/Llama-2-7b-chat/ \\\n","#     --tasks wikitext \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --load_quant quant_cache/Llama-2-7b-chat-w4-g128-awq-v2.pt\n","\n","# !python -m awq.entry --model_path /content/llm-awq/Efficient-Large-Model/VILA-13b/ \\\n","#     --tasks wikitext \\\n","#     --w_bit 4 --q_group_size 128 \\\n","#     --load_quant quant_cache/VILA-13b-w4-g128-awq-v2.pt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81190,"status":"ok","timestamp":1733676756859,"user":{"displayName":"Bryan Duong","userId":"08362964251007024437"},"user_tz":360},"id":"DNe-jM8IaitL","outputId":"db2673ef-eb0e-4637-c446-20d81066c6a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-12-08 16:51:19.408989: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-12-08 16:51:19.430221: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-12-08 16:51:19.436635: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-12-08 16:51:20.608064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Quantization config: {'zero_point': True, 'q_group_size': -1}\n","* Building model /content/llm-awq/facebook/opt-13b/\n","Loading checkpoint shards: 100% 3/3 [00:00<00:00,  3.80it/s]\n","evaluating...: 100% 140/140 [00:59<00:00,  2.35it/s]\n","10.127572059631348\n"]}],"source":["# FP16 evaluate the original model\n","# !python -m awq.entry --model_path /content/llm-awq/meta-llama/Meta-Llama-3-8B-Instruct/ \\\n","#     --tasks wikitext\n","\n","!python -m awq.entry --model_path /content/llm-awq/facebook/opt-13b/ \\\n","    --tasks wikitext\n","\n","# !python -m awq.entry --model_path /content/llm-awq/facebook/opt-6.7b/ \\\n","#     --tasks wikitext\n","\n","# !python -m awq.entry --model_path /content/llm-awq/facebook/opt-1.3b/ \\\n","#     --tasks wikitext\n","\n","# !python -m awq.entry --model_path /content/llm-awq/meta-llama/Llama-2-7b-chat/ \\\n","#       --tasks wikitext\n","\n","# !python -m awq.entry --model_path /content/llm-awq/Efficient-Large-Model/VILA-13b/ \\\n","#       --tasks wikitext"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14600,"status":"ok","timestamp":1733676852142,"user":{"displayName":"Bryan Duong","userId":"08362964251007024437"},"user_tz":360},"id":"r-kvYXQ2wECU","outputId":"e3feba62-a073-465c-a042-c3c1e152c404"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LLkSlmjw4nr"},"outputs":[],"source":["!rm -rf /content/llm-awq/meta-llama\n","!rm -rf /content/llm-awq/facebook\n","!rm -rf /content/llm-awq/Efficient-Large-Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpzVrcnywOqn"},"outputs":[],"source":["# !cp -r /content/llm-awq/ /content/drive/My\\ Drive/llm-awq-project\n","\n","# !cp /content/llm-awq/quant_cache/opt-6.7b-w4-g128-awq-v2.pt /content/drive/My\\ Drive/llm-awq-project/quant_cache/opt-6.7b-w4-g128-awq-v2.pt\n","# !cp /content/llm-awq/awq_cache/opt-6.7b-w4-g128.pt /content/drive/My\\ Drive/llm-awq-project/awq_cache/opt-6.7b-w4-g128.pt\n","\n","# !cp /content/llm-awq/awq_cache/opt-1.3b-w4-g128.pt /content/drive/My\\ Drive/llm-awq-project/awq_cache/opt-1.3b-w4-g128.pt\n","# !cp /content/llm-awq/quant_cache/opt-1.3b-w4-g128-awq-v2.pt /content/drive/My\\ Drive/llm-awq-project/quant_cache/opt-1.3b-w4-g128-awq-v2.pt\n","\n","!cp /content/llm-awq/awq_cache/opt-13b-w4-g128.pt /content/drive/My\\ Drive/llm-awq-project/awq_cache/opt-13b-w4-g128.pt\n","!cp /content/llm-awq/quant_cache/opt-13b-w4-g128-awq-v2.pt /content/drive/My\\ Drive/llm-awq-project/quant_cache/opt-13b-w4-g128-awq-v2.pt\n","\n","# !cp /content/llm-awq/quant_cache/Llama-2-7b-chat-w4-g128-awq-v2.pt /content/drive/My\\ Drive/llm-awq-project/quant_cache/Llama-2-7b-chat-w4-g128-awq-v2.pt\n","# !cp /content/llm-awq/awq_cache/Llama-2-7b-chat-w4-g128.pt /content/drive/My\\ Drive/llm-awq-project/awq_cache/Llama-2-7b-chat-w4-g128.pt"]},{"cell_type":"markdown","source":[],"metadata":{"id":"CP3cEBAJcZCX"}},{"cell_type":"markdown","source":[],"metadata":{"id":"yHwb4i_ncfTa"}},{"cell_type":"markdown","source":[],"metadata":{"id":"6SVAL5Mbcf4H"}},{"cell_type":"markdown","source":[],"metadata":{"id":"VHuTN-xqcgYq"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"authorship_tag":"ABX9TyM6ADCXAqMRS+tGWKNtZjE8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}